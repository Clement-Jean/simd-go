#include "textflag.h"

// func BitAndU8x16(a, b Uint8x16) Uint8x16
TEXT ·BitAndU8x16(SB),NOSPLIT,$0-48
	MOVD $a+0(FP), R0
	VLD1.P 32(R0), [V0.D2, V1.D2]
	VAND V1.B16, V0.B16, V0.B16
	MOVD $ret+32(FP), R0
	VST1.P [V0.D2], (R0)
	RET

// func BitAndU8x32(a, b Uint8x32) Uint8x32
TEXT ·BitAndU8x32(SB),NOSPLIT,$0-96
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VAND V2.B16, V0.B16, V0.B16
	VAND V3.B16, V1.B16, V1.B16
	MOVD $ret+64(FP), R0
	VST1.P [V0.D2, V1.D2], (R0)
	RET

// func BitAndU8x64(a, b Uint8x64) Uint8x64
TEXT ·BitAndU8x64(SB),NOSPLIT,$0-192
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VLD1.P 64(R0), [V4.D2, V5.D2, V6.D2, V7.D2]
	VAND V4.B16, V0.B16, V0.B16
	VAND V5.B16, V1.B16, V1.B16
	VAND V6.B16, V2.B16, V2.B16
	VAND V7.B16, V3.B16, V3.B16
	MOVD $ret+128(FP), R0
	VST1.P [V0.D2, V1.D2, V2.D2, V3.D2], (R0)
	RET

// func BitOrU8x16(a, b Uint8x16) Uint8x16
TEXT ·BitOrU8x16(SB),NOSPLIT,$0-48
	MOVD $a+0(FP), R0
	VLD1.P 32(R0), [V0.D2, V1.D2]
	VORR V1.B16, V0.B16, V0.B16
	MOVD $ret+32(FP), R0
	VST1.P [V0.D2], (R0)
	RET

// func BitOrU8x32(a, b Uint8x16) Uint8x16
TEXT ·BitOrU8x32(SB),NOSPLIT,$0-96
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VORR V2.B16, V0.B16, V0.B16
	VORR V3.B16, V1.B16, V1.B16
	MOVD $ret+64(FP), R0
	VST1.P [V0.D2, V1.D2], (R0)
	RET

// func BitOrU8x64(a, b Uint8x16) Uint8x16
TEXT ·BitOrU8x64(SB),NOSPLIT,$0-192
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VLD1.P 64(R0), [V4.D2, V5.D2, V6.D2, V7.D2]
	VORR V4.B16, V0.B16, V0.B16
	VORR V5.B16, V1.B16, V1.B16
	VORR V6.B16, V2.B16, V2.B16
	VORR V7.B16, V3.B16, V3.B16
	MOVD $ret+128(FP), R0
	VST1.P [V0.D2, V1.D2, V2.D2, V3.D2], (R0)
	RET

// func BitXorU8x16(a, b Uint8x16) Uint8x16
TEXT ·BitXorU8x16(SB),NOSPLIT,$0-48
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VEOR V1.B16, V0.B16, V0.B16
	MOVD $ret+32(FP), R0
	VST1.P [V0.D2], (R0)
	RET

// func BitXorU8x32(a, b Uint8x32) Uint8x32
TEXT ·BitXorU8x32(SB),NOSPLIT,$0-96
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	MOVD $ret+64(FP), R0
	VST1.P [V0.D2, V1.D2], (R0)
	RET

// func BitXorU8x64(a, b Uint8x64) Uint8x64
TEXT ·BitXorU8x64(SB),NOSPLIT,$0-192
	MOVD $a+0(FP), R0
	VLD1.P 64(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VLD1.P 64(R0), [V4.D2, V5.D2, V6.D2, V7.D2]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16
	MOVD $ret+128(FP), R0
	VST1.P [V0.D2, V1.D2, V2.D2, V3.D2], (R0)
	RET
